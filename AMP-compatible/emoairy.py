import torch
from torch.optim import Optimizer
import math

"""
AMPÂØæÂøúÂÆå‰∫Ü(202507) p.data -> p ‰øÆÊ≠£Ê∏à„Åø
memo : "optimizer = EmoAiry(model.parameters(), lr=1e-3, use_shadow=True)"
optimizer ÊåáÂÆö„ÅÆÈöõ„Å´ True / False „Åß shadow „ÇíÂàáÊõø„Åß„Åç„Çã(ÁèæÂú® False)
"""

class EmoAiry(Optimizer):
    # „ÇØ„É©„ÇπÂÆöÁæ©ÔºÜÂàùÊúüÂåñ - üî∏Shadow True(ÊúâÂäπ)/False(ÁÑ°Âäπ) ÂàáÊõø„Åà
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999),
                 eps=1e-8, weight_decay=0.01, use_shadow: bool = False): 
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)

        super().__init__(params, defaults)

        self.alpha_prev = getattr(self, 'alpha_prev', 1.0)
        self._init_lr = lr 
        self.should_stop = False # ÂÅúÊ≠¢„Éï„É©„Ç∞„ÅÆÂàùÊúüÂåñ
        self.use_shadow = use_shadow # üî∏shadow„ÅÆ‰ΩøÁî®„Éï„É©„Ç∞„Çí‰øùÂ≠ò

    # ÊÑüÊÉÖEMAÊõ¥Êñ∞(Á∑äÂºµ„Å®ÂÆâÈùô)
    def _update_ema(self, state, loss_val):
        ema = state.setdefault('ema', {})
        ema['short'] = 0.3 * loss_val + 0.7 * ema.get('short', loss_val)
        ema['long'] = 0.01 * loss_val + 0.99 * ema.get('long', loss_val)
        return ema

    # ÊÑüÊÉÖ„Çπ„Ç´„É©„ÉºÂÄ§ÁîüÊàê(EMAÂ∑ÆÂàÜ„ÄÅÊªë„Çâ„Åã„Å™ÈùûÁ∑öÂΩ¢„Çπ„Ç´„É©„Éº„ÄÅtanh 5 * diff „ÅßÈã≠Êïè„ÅïÂº∑Ë™ø)
    def _compute_scalar(self, ema):
        diff = ema['short'] - ema['long']
        return math.tanh(5 * diff)

    # ShadowÊ∑∑ÂêàÊØîÁéá(> 0.6Ôºö70„Äú90%„ÄÅ < -0.6Ôºö10%„ÄÅ abs> 0.3Ôºö30%„ÄÅ Âπ≥ÊôÇÔºö0%)
    def _decide_ratio(self, scalar):
        # üî∏use_shadow „Åå False „ÅÆÂ†¥Âêà„ÅØÂ∏∏„Å´ÊØîÁéá„Çí 0 „Å´„Åô„Çã
        if not self.use_shadow:
            return 0.0
        if scalar > 0.6:
            return 0.7 + 0.2 * scalar
        elif scalar < -0.6:
            return 0.1
        elif abs(scalar) > 0.3:
            return 0.3
        return 0.0

    # ÊêçÂ§±ÂèñÂæó(ÊêçÂ§±ÂÄ§ loss_val „ÇíÊï∞ÂÄ§Âåñ„ÄÅÊÑüÊÉÖÂà§ÂÆö„Å´‰ΩøÁî®„ÄÅÂ≠òÂú®„Åó„Å™„ÅÑ„Éë„É©„É°„Éº„Çø(Êõ¥Êñ∞‰∏çË¶Å)„ÅØ„Çπ„Ç≠„ÉÉ„Éó)
    @torch.no_grad()
    def step(self, closure=None):
        loss = closure() if closure is not None else None
        loss_val = loss.item() if loss is not None else 0.0

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue

                grad = p.grad
                state = self.state[p]

                # ÊÑüÊÉÖEMAÊõ¥Êñ∞„Éª„Çπ„Ç´„É©„ÉºÁîüÊàê (Êó¢Â≠ò„É≠„Ç∏„ÉÉ„ÇØ„ÇíÁ∂≠ÊåÅ)
                ema = self._update_ema(state, loss_val)
                scalar = self._compute_scalar(ema)
                ratio = self._decide_ratio(scalar) # üî∏use_shadow „Å´Âøú„Åò„Å¶ ratio „Åå 0 „Å´„Å™„Çã

                # shadow_paramÔºöÂøÖË¶ÅÊôÇ„ÅÆ„ÅøÊõ¥Êñ∞ (Êó¢Â≠ò„É≠„Ç∏„ÉÉ„ÇØ„ÇíÁ∂≠ÊåÅ)
                # üî∏self.use_shadow „Åå True „Åß„ÄÅ„Åã„Å§ ratio > 0 „ÅÆÂ†¥Âêà„ÅÆ„Åø shadow „ÇíÊõ¥Êñ∞
                if self.use_shadow and ratio > 0: 
                    if 'shadow' not in state:
                        state['shadow'] = p.clone()
                    else:
                        p.mul_(1 - ratio).add_(state['shadow'], alpha=ratio)
                        state['shadow'].lerp_(p, 0.05)

                # --- ÂãæÈÖçË£úÊ≠£„É≠„Ç∏„ÉÉ„ÇØ ---
                # Ë°åÂàó„ÅÆÂΩ¢Áä∂„Åå2Ê¨°ÂÖÉ‰ª•‰∏ä„ÅÆÂ†¥Âêà„ÄÅÂàÜÊï£ÊÉÖÂ†±„Éô„Éº„Çπ„ÅÆABËøë‰ºº„Çí‰ΩøÁî®
                if grad.dim() >= 2:
                    # „Éï„Ç£„É´„Çø„Éº„Åó„Åç„ÅÑÂÄ§ÔºàÊé¢Á¥¢Âº∑Â∫¶Ôºâ‚ÜêË™øÊï¥ÂèØËÉΩ
                    threshold = 1e-4 * (1 + abs(scalar))

                    # Ë°å„Å®Âàó„ÅÆ2‰πóÂπ≥Âùá„ÇíË®àÁÆó (ÂàÜÊï£„ÅÆËªΩÈáè„Å™Ëøë‰ºº)
                    r_sq = torch.mean(grad * grad, dim=tuple(range(1, grad.dim())), keepdim=True).add_(group['eps'])
                    c_sq = torch.mean(grad * grad, dim=0, keepdim=True).add_(group['eps'])

                    # Ë°åÊñπÂêë/ÂàóÊñπÂêë Êé¢Á¥¢„Éï„Ç£„É´„Çø„Éº
                    r_mask = (r_sq.pow(1/3) > threshold).float()  # Ë°åÊñπÂêë„Éû„Çπ„ÇØ
                    c_mask = (c_sq.pow(1/3) > threshold).float()  # ÂàóÊñπÂêë„Éû„Çπ„ÇØ

                    # Ë°å„Å®Âàó„ÅÆ„Éû„Çπ„ÇØ„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Å¶„ÄÅ„Éë„É©„É°„Éº„Çø„Åî„Å®„ÅÆÊúÄÁµÇÁöÑ„Å™„Éû„Çπ„ÇØ„Çí‰ΩúÊàê
                    # torch.matmul„ÅØ2Ê¨°ÂÖÉ„ÉÜ„É≥„ÇΩ„É´„ÇíÂâçÊèê„Å®„Åô„Çã„Åü„ÇÅ„ÄÅÂÖÉ„ÅÆ„Ç≥„Éº„Éâ„ÅÆ„É≠„Ç∏„ÉÉ„ÇØ„Çí‰øÆÊ≠£
                    update_mask = r_mask * c_mask

                    # AdafactorÁöÑ„Å™Êõ¥Êñ∞È†Ö„ÇíË®àÁÆó
                    beta1, beta2 = group['betas']
                    eps = group['eps']

                    # EMA„ÅßÂπ≥ÊªëÂåñ„Åï„Çå„ÅüË°å„Å®Âàó„ÅÆÂàÜÊï£„ÇíË®àÁÆóÔºàÂÖÉ„ÅÆ„Ç≥„Éº„Éâ„ÅÆdenomÈÉ®ÂàÜÔºâ
                    state.setdefault('exp_avg_r', torch.zeros_like(r_sq)).mul_(beta1).add_(torch.sqrt(r_sq), alpha=1 - beta1)
                    state.setdefault('exp_avg_c', torch.zeros_like(c_sq)).mul_(beta1).add_(torch.sqrt(c_sq), alpha=1 - beta1)

                    # ÂÜçÊßãÁØâ„Åó„ÅüËøë‰ººÂãæÈÖç„ÅÆÂπ≥ÊñπÊ†π„ÅÆÁ©ç„ÅßÊ≠£Ë¶èÂåñ
                    # „Åì„Çå„Å´„Çà„Çä2Ê¨°„É¢„Éº„É°„É≥„Éà„ÅÆ„Çà„ÅÜ„Å™ÂΩπÂâ≤„ÇíÊûú„Åü„Åô
                    denom = torch.sqrt(state['exp_avg_r'] * state['exp_avg_c']) + eps

                    # ÂãæÈÖçÊõ¥Êñ∞È†Ö„ÅÆÈÅ∏Âà• ÈÄöÂ∏∏„ÅÆgrad/denom„ÅÆÊõ¥Êñ∞Âºè„Å´ÂØæ„Åó„ÄÅ‰∏ä„Åß‰ΩúÊàê„Åó„Åü„Éû„Çπ„ÇØ„ÇíÈÅ©Áî®
                    update_term = (grad / denom) * update_mask

                # 1Ê¨°ÂÖÉ(„Éô„ÇØ„Éà„É´)„ÅÆÂãæÈÖçË£úÊ≠£(decoupled weight decay ÊßãÈÄ†„Å´Ëøë„ÅÑ)
                else:
                    # 3‰πóÂπ≥ÊñπÊ†π„Å´„Çà„Çã„Éï„Ç£„É´„Çø„Éº„ÇíÈÅ©Áî®
                    # „Éï„Ç£„É´„Çø„Éº„ÅÆÂº∑Â∫¶„ÇíÊ±∫ÂÆö„Åô„Çã„Åó„Åç„ÅÑÂÄ§„ÇíË®≠ÂÆö
                    # „Åì„Åì„Åß„ÅØ‰æã„Å®„Åó„Å¶1e-4„Çí‰ΩøÁî®„Åó„Åæ„Åô„Åå„ÄÅ„Åì„Çå„ÅØË™øÊï¥ÂèØËÉΩ„Åß„Åô
                    threshold = 1e-4 * (1 + abs(scalar))

                    exp_avg = state.setdefault('exp_avg', torch.zeros_like(p))
                    exp_avg_sq = state.setdefault('exp_avg_sq', torch.zeros_like(p))
                    beta1, beta2 = group['betas']

                    # Adam„ÅÆ1Ê¨°„É¢„Éº„É°„É≥„Éà„Å®2Ê¨°„É¢„Éº„É°„É≥„Éà„ÇíË®àÁÆó
                    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)

                    # ÈÄöÂ∏∏„ÅÆAdam„ÅÆÊõ¥Êñ∞È†Ö„ÇíË®àÁÆó
                    denom = exp_avg_sq.sqrt().add_(group['eps'])
                    update_term = exp_avg / denom

                    # ÂãæÈÖç„ÅÆ3‰πóÂπ≥ÊñπÊ†π„Åå„Åó„Åç„ÅÑÂÄ§„ÇíË∂Ö„Åà„ÇãÈÉ®ÂàÜ„Çí„Éû„Çπ„ÇØ„Å®„Åó„Å¶ÊäΩÂá∫
                    filter_mask = (grad.pow(2).pow(1/3) > threshold).float()

                    # Êõ¥Êñ∞È†Ö„Å´„Éû„Çπ„ÇØ„ÇíÈÅ©Áî®„Åó„Å¶ÈÅ∏Âà•
                    update_term = update_term * filter_mask

                # ÊúÄÁµÇÁöÑ„Å™„Éë„É©„É°„Éº„ÇøÊõ¥Êñ∞ (decoupled weight decay„ÇÇÈÅ©Áî®)
                p.add_(p, alpha=-group['weight_decay'] * group['lr'])
                p.add_(update_term, alpha=-group['lr'] * (1 - abs(scalar)))

                # --- Early Stop „É≠„Ç∏„ÉÉ„ÇØ (Êó¢Â≠ò„É≠„Ç∏„ÉÉ„ÇØ„ÇíÁ∂≠ÊåÅ) ---
                hist = self.state.setdefault('scalar_hist', [])
                hist.append(scalar)
                if len(hist) >= 33:
                    hist.pop(0)

        # Early StopÂà§Êñ≠
        if len(self.state['scalar_hist']) >= 32:
            buf = self.state['scalar_hist']
            avg_abs = sum(abs(s) for s in buf) / len(buf)
            std = sum((s - sum(buf)/len(buf))**2 for s in buf) / len(buf)
            if avg_abs < 0.05 and std < 0.005:
                self.should_stop = True

        return loss

"""
 https://github.com/muooon/EmoNavi
 Airy is inspired by Adafactor, and EmoFact,  
 and its VRAM-friendly design is something everyone loves.
"""