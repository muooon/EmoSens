import torch
from torch.optim import Optimizer
import math

"""
EmoSens v2.0 (250815) shadow-system v2.0 shadow-effect v1.0
AMPÂØæÂøúÂÆå‰∫Ü(202507) p.data -> p ‰øÆÊ≠£Ê∏à„Åø
memo : "optimizer = EmoSens(model.parameters(), lr=1e-3, use_shadow=True)"
optimizer ÊåáÂÆö„ÅÆÈöõ„Å´ True / False „Åß shadow „ÇíÂàáÊõø„Åß„Åç„Çã(ÁèæÂú® False)
"""

class EmoSens(Optimizer):
    # „ÇØ„É©„ÇπÂÆöÁæ©ÔºÜÂàùÊúüÂåñ üî∏Shadow True(ÊúâÂäπ)/False(ÁÑ°Âäπ) ÂàáÊõø„Åà
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999),
                 eps=1e-8, weight_decay=0.01, use_shadow: bool = False):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        super().__init__(params, defaults)
        self._init_lr = lr 
        self.should_stop = False # ÂÅúÊ≠¢„Éï„É©„Ç∞„ÅÆÂàùÊúüÂåñ
        self.use_shadow = use_shadow # üî∏shadow„ÅÆ‰ΩøÁî®„Éï„É©„Ç∞„Çí‰øùÂ≠ò

    # ÊÑüÊÉÖEMAÊõ¥Êñ∞(Á∑äÂºµ„Å®ÂÆâÈùô)
    def _update_ema(self, state, loss_val):
        ema = state.setdefault('ema', {})
        ema['short'] = 0.3 * loss_val + 0.7 * ema.get('short', loss_val)
        ema['long'] = 0.01 * loss_val + 0.99 * ema.get('long', loss_val)
        return ema

    # ÊÑüÊÉÖ„Çπ„Ç´„É©„ÉºÂÄ§ÁîüÊàê(EMAÂ∑ÆÂàÜ„ÄÅÊªë„Çâ„Åã„Å™ÈùûÁ∑öÂΩ¢„Çπ„Ç´„É©„Éº„ÄÅtanh 5 * diff „ÅßÈã≠Êïè„ÅïÂº∑Ë™ø)
    def _compute_scalar(self, ema):
        diff = ema['short'] - ema['long']
        return math.tanh(5 * diff)

    # ShadowÊ∑∑ÂêàÊØîÁéá(> abs 0.6Ôºö60„Äú100%„ÄÅ > abs 0.1Ôºö10„Äú60%„ÄÅ Âπ≥ÊôÇÔºö0%) emosensÂèçÊò†
    # ÊóßÔºöShadowÊ∑∑ÂêàÊØîÁéá(> 0.6Ôºö80„Äú90%„ÄÅ < -0.6Ôºö10%„ÄÅ abs> 0.3Ôºö30%„ÄÅ Âπ≥ÊôÇÔºö0%)
    # Ë™¨ÊòéÔºöscalar>+0.6 „ÅØ "return 0.7(ÈñãÂßãÂÄ§) + 0.2(Â§âÂåñÂπÖ) * scalar" = 0.82ÔΩû0.9 ‚Üê Ë™§
    # ‰øÆÊ≠£1Ôºöscalar>¬±0.6 „Çí "return ÈñãÂßãÂÄ§ + (abs(scalar) - 0.6(ÁØÑÂõ≤)) / ÁØÑÂõ≤Èáè * Â§âÂåñÂπÖ"
    # ‰øÆÊ≠£2Ôºöscalar>¬±0.1 „Çí "return ÈñãÂßãÂÄ§ + (abs(scalar) - 0.1(ÁØÑÂõ≤)) / ÁØÑÂõ≤Èáè * Â§âÂåñÂπÖ"
    # „Çø„Çπ„ÇØÁ≠â„Å´Âøú„Åò„ÅüË™øÊï¥„ÅÆ„Åü„ÇÅÔºìÊÆµÈöé„ÅßÈÅ©Áî®„Åó„Å¶„Åä„Åè(‰∏äË®ò„ÇíÂèÇËÄÉ„Å´Ë™øÊï¥„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºèÁèæÁä∂„ÅØshadow-effectÂèçÊò†)
    def _decide_ratio(self, scalar):
        if not self.use_shadow:
            return 0.0 # üî∏use_shadow „Åå False „ÅÆÂ†¥Âêà„ÅØÂ∏∏„Å´ÊØîÁéá„Çí 0 „Å´„Åô„Çã
        if abs(scalar) > 0.6:
            return 0.6 + (abs(scalar) - 0.6) / 0.4 * 0.4 # ÂÖÉ return 0.7 + 0.2 * scalar
        elif abs(scalar) > 0.1:
            return 0.1 + (abs(scalar) - 0.1) / 0.5 * 0.5 # ÂÖÉ return 0.3
        return 0.0

    # ÊêçÂ§±ÂèñÂæó(ÊêçÂ§±ÂÄ§ loss_val „ÇíÊï∞ÂÄ§Âåñ„ÄÅÊÑüÊÉÖÂà§ÂÆö„Å´‰ΩøÁî®„ÄÅÂ≠òÂú®„Åó„Å™„ÅÑ„Éë„É©„É°„Éº„Çø(Êõ¥Êñ∞‰∏çË¶Å)„ÅØ„Çπ„Ç≠„ÉÉ„Éó)
    @torch.no_grad()
    def step(self, closure=None):
        loss = closure() if closure is not None else None
        loss_val = loss.item() if loss is not None else 0.0

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue

                grad = p.grad
                state = self.state[p]

                # EMAÊõ¥Êñ∞„Éª„Çπ„Ç´„É©„ÉºÁîüÊàê(EMAÂ∑ÆÂàÜ„Åã„Çâ„Çπ„Ç´„É©„Éº„ÇíÁîüÊàê„Åó„Çπ„Éë„Ç§„ÇØÊØîÁéá„ÇíÊ±∫ÂÆö)
                ema = self._update_ema(state, loss_val)
                scalar = self._compute_scalar(ema)
                ratio = self._decide_ratio(scalar) # üî∏use_shadow „Å´Âøú„Åò„Å¶ ratio „Åå 0 „Å´„Å™„Çã

                # üî∏self.use_shadow „Åå True „Åß„ÄÅ„Åã„Å§ ratio > 0 „ÅÆÂ†¥Âêà„ÅÆ„Åø shadow „ÇíÊõ¥Êñ∞
                if self.use_shadow and ratio > 0: 
                    if 'shadow' not in state:
                        state['shadow'] = p.clone()
                    else:
                        p.mul_(1 - ratio).add_(state['shadow'], alpha=ratio)
                        state['shadow'].lerp_(p, 0.05)
                
                # „Çπ„Ç´„É©„ÉºÁîüÊàêÔºöÁü≠Êúü„Å®Èï∑ÊúüEMA„ÅÆÂ∑ÆÂàÜ„Åã„Çâ‰ø°Âè∑„ÇíÂæó„Çã(È´ò„Å∂„Çä„ÅÆÂº∑„Åï)
                # Ê∑∑ÂêàÊØîÁéáÔºö„Çπ„Ç´„É©„Éº„ÅåÈñæÂÄ§„ÇíË∂Ö„Åà„ÇãÂ†¥Âêà„Å´„ÅÆ„ÅøË®àÁÆó„Åï„Çå„Çã(‰ø°È†º„Åß„Åç„ÇãÊÑüÊÉÖ‰ø°Âè∑„Åã„Å©„ÅÜ„Åã„ÅÆÈÅ∏Âà•)
                # ‚Üí „Çπ„Ç´„É©„ÉºÂÄ§„ÅåÂ∞è„Åï„ÅÑÂ†¥Âêà„ÅØ ratio = 0 „Å®„Å™„Çä„ÄÅshadowÊ∑∑Âêà„ÅØË°å„Çè„Çå„Å™„ÅÑ
                # ‚Üí ‰ø°È†º„Åß„Åç„ÇãÂº∑„ÅÑÂ∑ÆÂàÜ„ÅÆ„Å®„Åç„ÅÆ„ÅøÊÑüÊÉÖÊ©üÊßã„ÅåÁô∫Âãï„Åô„Çã(ÊöóÈªô„ÅÆ‰ø°È†ºÂ∫¶Âà§ÂÆö)                
                
                # 1Ê¨°„Éª2Ê¨°„É¢„Éº„É°„É≥„Éà„Çí‰Ωø„Å£„ÅüÂãæÈÖçË£úÊ≠£(decoupled weight decay ÊßãÈÄ†„Å´Ëøë„ÅÑ)
                exp_avg = state.setdefault('exp_avg', torch.zeros_like(p))
                exp_avg_sq = state.setdefault('exp_avg_sq', torch.zeros_like(p))
                beta1, beta2 = group['betas']

                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
                denom = exp_avg_sq.sqrt().add_(group['eps'])

                # ÂãæÈÖç„ÅÆÂ§ö‰πóÊ†π„ÇíË®àÁÆó„Åó„Å¶„ÄÅ„Éï„Ç£„É´„Çø„Éº„ÅÆÂü∫Ê∫ñ„Å®„Åô„Çã
                threshold = 1e-4 * (1 + abs(scalar))
                filter_strength = torch.abs(grad).pow(1/3)

                # „Éï„Ç£„É´„Çø„É™„É≥„Ç∞Âº∑Â∫¶„Åå„Åó„Åç„ÅÑÂÄ§„ÇíË∂Ö„Åà„ÅüÈÉ®ÂàÜ„ÅåTrue„Å´„Å™„Çã
                mask = torch.ge(filter_strength, threshold).float()

                # Êõ¥Êñ∞Èáè„ÇíË®àÁÆó
                update_term = exp_avg.div(denom)

                # Êõ¥Êñ∞Èáè„Å´„Éû„Çπ„ÇØ„ÇíÈÅ©Áî®„Åó„Å¶„ÄÅÁîü„ÅçÊÆã„Å£„ÅüÈÉ®ÂàÜ„ÅÆ„Åø„ÇíÊõ¥Êñ∞
                filtered_update = update_term * mask

                # ÊúÄÁµÇÁöÑ„Å™„Éë„É©„É°„Éº„ÇøÊõ¥Êñ∞Ôºàdecoupled weight decay„ÇÇÈÅ©Áî®Ôºâ
                if group['weight_decay']:
                    p.add_(p, alpha=-group['weight_decay'] * group['lr'])
                p.add_(filtered_update, alpha=-group['lr'] * (1 - abs(scalar)))

        # ÊÑüÊÉÖÊ©üÊßã„ÅÆÁô∫ÁÅ´„ÅåÂèé„Åæ„Çä"ÂçÅÂàÜ„Å´ÂÆâÂÆö"„Åó„Å¶„ÅÑ„Çã„Åì„Å®„ÇíÂ§ñÈÉ®‰ºùÈÅî„Åß„Åç„Çã(Ëá™ÂãïÂÅúÊ≠¢„É≠„Ç∏„ÉÉ„ÇØ„Åß„ÅØ„Å™„ÅÑ)
                # Early StopÁî® scalar Ë®òÈå≤(„Éê„ÉÉ„Éï„Ç°ÂÖ±ÈÄö„ÅßÁÆ°ÁêÜ/ÊúÄÂ§ß32‰ª∂‰øùÊåÅ/ÂãïÈùôË©ï‰æ°)
                hist = self.state.setdefault('scalar_hist', [])
                hist.append(scalar)
                if len(hist) >= 33:
                    hist.pop(0)

        # Early StopÂà§Êñ≠(Èùô„Åë„Åï„ÅÆÂêàÂõ≥)
        if len(self.state['scalar_hist']) >= 32:
            buf = self.state['scalar_hist']
            avg_abs = sum(abs(s) for s in buf) / len(buf)
            std = sum((s - sum(buf)/len(buf))**2 for s in buf) / len(buf)
            if avg_abs < 0.05 and std < 0.005:
                self.should_stop = True  # üí° Â§ñÈÉ®„Åã„Çâ„Åì„Çå„ÇíË¶ã„Å¶Âà§Êñ≠ÂèØ

        # 32„Çπ„ÉÜ„ÉÉ„ÉóÂàÜ„ÅÆ„Çπ„Ç´„É©„ÉºÂÄ§„ÅÆÈùô„Åã„Å™Êù°‰ª∂„ÇíÊ∫Ä„Åü„Åó„ÅüÊôÇ"„Éï„É©„Ç∞" should_stop = True „Å´„Å™„Çã„Å†„Åë

        return loss

"""
 https://github.com/muooon/EmoNavi
 An emotion-driven optimizer that feels loss and navigates accordingly.
 Don't think. Feel. Don't stop. Keep running. Believe in what's beyond.
"""
